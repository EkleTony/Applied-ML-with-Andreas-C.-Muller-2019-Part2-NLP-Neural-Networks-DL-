{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "* Word Embedding is a technique of word representation that allows words with similar meaning to be understood by machine learning algorithms. Technically speaking, it is a mapping of words into vectors of real numbers using the neural network, probabilistic model, or dimension reduction on word co-occurrence matrix.\n",
    "\n",
    "* Word embeddings are also very useful in mitigating the curse of dimensionality, a very recurring problem in artificial intelligence. Without word embedding, the unique identifiers representing the words generate scattered data, isolated points in a vast sparse representation. With word embedding, on the other hand, the space becomes much more limited in terms of dimensionality with a widely richer amount of semantic information. With such numerical features, it is easier for a computer to perform different mathematical operations like matrix factorization, dot product, etc. which are mandatory to use shallow and deep learning techniques. There are many techniques available at our disposal to achieve this transformation. In this article, we will be covering: \n",
    "  1. Bag-Of-Words: The grammar and word order are neglected while the frequency is kept the same.\n",
    "  \n",
    "  2. TF-ID: The Term Frequency-Inverse Document Frequency (a.k.a. TF-IDF) is another way to represent a document based on its words. With TF-IDF, words are given weights by TF-IDF importance instead of only frequency. The weight increases in proportion to the number of occurrences of the word in the document. It also varies according to the frequency of the word in the corpus. \n",
    "* By definition, TF-IDF embedding is composed by two terms: the first computes the normalized **Term Frequency** (TF), a.k.a. the occurrence a word appears in a document, divided by the total number of words in that document; the second term is the **Inverse Document Frequency** (IDF) which computes the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "  \n",
    "  3. Word2Vec: One of the most efficient techniques to represent a word is Word2Vec. Word2vec is a computationally efficient predictive model for learning word embeddings from raw text. It plots the words in a multi-dimensional vector space, where similar words tend to be close to each other. The surrounding words of a word provide the context to that word.\n",
    "* Word2Vec can rely on either one of two model architectures in order to produce a distributed representation of input words: **Continuous Bag-of-Words (CBoW) or Continuous Skip-Gram** as shown in the figure below. Vector representation extracts semantic relationships based on the co-occurrence of words in the dataset.\n",
    "\n",
    "* The CBoW and skip-gram models are trained using a binary classification to discriminate between the real target word and the other words in the same context. The accuracy at which the model predicts the words depends on how many times the model sees these words within the same context throughout the dataset. The hidden representation is changed by more words and context co-occurrences during the training process, which allows the model to have more future successful predictions, leading to a better representation of word and context in the vector space. Skip gram is much slower than CBOW, but performs more accurately with infrequent words.\n",
    "  4. Doc2vec and Doc2vecC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\user\\anaconda3\\envs\\tensorflow\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading gensim-3.8.3-cp36-cp36m-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from gensim) (1.19.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.2.0.tar.gz (119 kB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\user\\appdata\\roaming\\python\\python36\\site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\user\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp36-cp36m-win_amd64.whl (1.7 MB)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-4.2.0-py3-none-any.whl size=109637 sha256=8074f14648a1ce95b7cd7427acaef0e314bf4c36d7d212280f05cb3ff42d5c78\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\05\\12\\87\\d479d6a8f92130cd8b27e331cc433bb28dda9c20e57f0b1ab2\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "Successfully installed Cython-0.29.14 gensim-3.8.3 smart-open-4.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training own word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [['Hello', 'This', 'is', 'python','training','by','Tony'],\n",
    "                      ['Hello', 'This', 'is', 'Java','training','by','Tony'],\n",
    "                      ['Hello', 'This', 'is', 'Data Science','training','by','Tony'],\n",
    "                      ['Hello', 'This', 'is', 'Progamming','training','by','Tony']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'This', 'is', 'python', 'training', 'by', 'Tony'],\n",
       " ['Hello', 'This', 'is', 'Java', 'training', 'by', 'Tony'],\n",
       " ['Hello', 'This', 'is', 'Data Science', 'training', 'by', 'Tony'],\n",
       " ['Hello', 'This', 'is', 'Progamming', 'training', 'by', 'Tony']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training word2vec model\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "#warnings.filterwarning('ignore')\n",
    "mymodel = Word2Vec(tokenized_sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=10, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize vocabulary\n",
    "words = list(mymodel.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'This',\n",
       " 'is',\n",
       " 'python',\n",
       " 'training',\n",
       " 'by',\n",
       " 'Tony',\n",
       " 'Java',\n",
       " 'Data Science',\n",
       " 'Progamming']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.8681172e-03  1.1729884e-03  2.8752410e-03 -2.6626838e-03\n",
      " -3.7820984e-03  5.8760698e-04 -2.5913378e-03  2.3307244e-04\n",
      " -2.3304853e-03 -3.4849791e-04  7.2109944e-04  4.6286588e-03\n",
      " -9.4307517e-04 -1.7961187e-03  4.6753464e-03 -3.9190808e-03\n",
      "  3.8409515e-03  8.9049798e-05  4.3157525e-03 -2.5745116e-03\n",
      " -3.9713527e-03 -4.8930417e-03 -2.3911954e-03  3.5810263e-03\n",
      "  1.8859054e-03  2.9160185e-03  1.1615632e-03  4.7634281e-03\n",
      "  1.2557892e-03 -2.7929787e-03  8.7547407e-04 -3.2141067e-03\n",
      " -1.3748390e-04  1.9690676e-03 -1.9810899e-04  1.6533141e-03\n",
      "  4.9852738e-03 -3.0008946e-03  2.3838012e-03 -1.6623344e-03\n",
      "  4.8859636e-03 -4.4806344e-03  1.3658563e-03 -3.1863546e-04\n",
      "  3.8947506e-04 -2.1078769e-04  3.6475965e-04 -4.7123926e-03\n",
      "  2.9228902e-03 -2.3194427e-04 -2.1781977e-03 -4.4255052e-03\n",
      "  3.5965063e-03  2.6320957e-03  8.5198320e-04 -1.8269044e-03\n",
      "  2.8597277e-03  2.0904257e-03 -1.9068263e-03 -1.5578708e-03\n",
      "  3.9975457e-03 -1.7435518e-03  2.0039666e-03 -3.6953052e-03\n",
      "  1.5547335e-03  4.7626980e-03  5.7082332e-04 -4.0961248e-03\n",
      " -4.3715704e-03 -2.2977607e-03 -1.4587244e-03 -1.9207400e-03\n",
      " -4.1833003e-03 -1.3180929e-03  6.1490521e-04 -1.6805127e-03\n",
      "  1.2236520e-03  4.3033808e-03 -3.8037000e-03 -1.9544442e-03\n",
      "  2.8530722e-03  9.3934953e-04 -1.4499074e-04  1.5341780e-04\n",
      "  5.1457126e-04  4.0036933e-03  1.0310145e-03  1.3978095e-03\n",
      "  3.2797719e-03  5.9451495e-04 -1.9943959e-03  2.9610426e-03\n",
      " -3.1089690e-03  3.7029579e-03 -2.9268004e-03  3.2695439e-03\n",
      "  4.7370137e-04  8.8791212e-04  6.8547582e-05 -2.8114489e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# access word vectors for one word training\n",
    "print(mymodel['Hello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Tony', 0.2056553065776825),\n",
       " ('python', 0.07027705013751984),\n",
       " ('This', 0.02738291770219803),\n",
       " ('is', 0.023710981011390686),\n",
       " ('Progamming', -0.043045032769441605),\n",
       " ('training', -0.08911749720573425),\n",
       " ('Data Science', -0.10308177769184113),\n",
       " ('by', -0.12382837384939194),\n",
       " ('Java', -0.2511283755302429)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try finding most similar words for word 'data'\n",
    "mymodel.most_similar('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try other words not in word2vec model\n",
    "\n",
    "#mymodel.most_similar('Good')\n",
    "\n",
    "# KeyError: \"word 'Good' not in vocabulary\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a great limitation in training your own modle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embedding model using Kerass Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.version.VERSION\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding from scratch\n",
    "sent = ['Hello, how are you', 'how are you',\n",
    "       'how are you doing', 'I am doing great',\n",
    "       'I am doing good', 'I am good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining class labels\n",
    "\n",
    "sent_labels = array([1,1,1,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 17, 28, 10], [17, 28, 10], [17, 28, 10, 20], [29, 16, 20, 7], [29, 16, 20, 4], [29, 16, 4]]\n"
     ]
    }
   ],
   "source": [
    "#integer encoding of the documents\n",
    "my_vocab_size = 30\n",
    "encoded_sent = [one_hot(i, my_vocab_size) for i in sent]\n",
    "print(encoded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 20 17 28 10]\n",
      " [ 0  0 17 28 10]\n",
      " [ 0 17 28 10 20]\n",
      " [ 0 29 16 20  7]\n",
      " [ 0 29 16 20  4]\n",
      " [ 0  0 29 16  4]]\n"
     ]
    }
   ],
   "source": [
    "# padding documents to a max length = 5\n",
    "\n",
    "length = 5\n",
    "padded_sent = pad_sequences(encoded_sent, maxlen=length, padding ='pre')\n",
    "print(padded_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# defining a NN  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel2 = Sequential()\n",
    "mymodel2.add(Embedding(my_vocab_size, 8, input_length=length))\n",
    "mymodel2.add(Flatten())\n",
    "mymodel2.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model\n",
    "mymodel2.compile(optimizer='adam', loss ='binary_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6 samples\n",
      "Epoch 1/30\n",
      "6/6 [==============================] - 0s 499us/sample - loss: 0.3434 - accuracy: 1.0000\n",
      "Epoch 2/30\n",
      "6/6 [==============================] - 0s 500us/sample - loss: 0.3403 - accuracy: 1.0000\n",
      "Epoch 3/30\n",
      "6/6 [==============================] - 0s 500us/sample - loss: 0.3372 - accuracy: 1.0000\n",
      "Epoch 4/30\n",
      "6/6 [==============================] - 0s 500us/sample - loss: 0.3341 - accuracy: 1.0000\n",
      "Epoch 5/30\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3310 - accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3280 - accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3249 - accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "6/6 [==============================] - 0s 500us/sample - loss: 0.3219 - accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "6/6 [==============================] - 0s 665us/sample - loss: 0.3189 - accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "6/6 [==============================] - 0s 1ms/sample - loss: 0.3159 - accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "6/6 [==============================] - 0s 500us/sample - loss: 0.3130 - accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "6/6 [==============================] - 0s 499us/sample - loss: 0.3100 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.3071 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "6/6 [==============================] - 0s 498us/sample - loss: 0.3042 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "6/6 [==============================] - 0s 500us/sample - loss: 0.3013 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "6/6 [==============================] - 0s 334us/sample - loss: 0.2984 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "6/6 [==============================] - 0s 1ms/sample - loss: 0.2955 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2927 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "6/6 [==============================] - 0s 500us/sample - loss: 0.2899 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "6/6 [==============================] - 0s 499us/sample - loss: 0.2871 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2843 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "6/6 [==============================] - 0s 331us/sample - loss: 0.2816 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2788 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "6/6 [==============================] - 0s 500us/sample - loss: 0.2761 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2734 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2707 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "6/6 [==============================] - 0s 500us/sample - loss: 0.2681 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "6/6 [==============================] - 0s 334us/sample - loss: 0.2654 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "6/6 [==============================] - 0s 500us/sample - loss: 0.2628 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "6/6 [==============================] - 0s 500us/sample - loss: 0.2602 - accuracy: 1.0000\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# fiting the model\n",
    "mymodel2.fit(padded_sent, sent_labels, epochs=30)\n",
    "\n",
    "mymodelloss, mymodelaccuracy = mymodel2.evaluate(padded_sent, sent_labels, verbose=0 )\n",
    "print('Accuracy: %f' % (mymodelaccuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysent_to_predict = ['how are  you Tony', 'I am super good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17, 28, 10, 17], [29, 16, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = 30\n",
    "encoded =  [one_hot(d, vocab_size) for d in mysent_to_predict]\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 17 28 10 17]\n",
      " [ 0 29 16  1  4]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 5\n",
    "mypadded = pad_sequences(encoded, maxlen=max_length, padding='pre')\n",
    "print(mypadded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel2.predict_classes(mypadded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
